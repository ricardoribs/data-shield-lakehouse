from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans # Usaremos KMeans para simplificar (agrupa comportamentos)

# --- CONFIGURA√á√ÉO ---
spark = SparkSession.builder \
    .appName("DataShield-ML-Training") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "admin") \
    .config("spark.hadoop.fs.s3a.secret.key", "password") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

print("ü§ñ Iniciando Treinamento de Modelo de Detec√ß√£o de Anomalias...")

try:
    # 1. Ler dados da Silver (Transa√ß√µes detalhadas)
    print("üì• Carregando dados da Silver...")
    df = spark.read.format("delta").load("s3a://silver/transactions")
    
    # 2. Preparar Features (Vamos usar o Valor da Transa√ß√£o para detectar anomalias)
    assembler = VectorAssembler(inputCols=["amount"], outputCol="features")
    df_features = assembler.transform(df)

    # 3. Treinar Modelo (K-Means com 2 clusters: "Normal" e "Alto Valor/Suspeito")
    # Em um cen√°rio real, usariamos IsolationForest, mas o KMeans √© nativo e r√°pido no Spark
    print("üß† Treinando modelo K-Means...")
    kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol("features")
    model = kmeans.fit(df_features)

    # 4. Fazer Predi√ß√µes
    predictions = model.transform(df_features)

    # 5. Identificar qual cluster tem a m√©dia maior (o cluster de "Alto Valor")
    centers = model.clusterCenters()
    # O cluster com o centroide maior √© o de transa√ß√µes altas
    high_value_cluster = 0 if centers[0][0] > centers[1][0] else 1
    
    print(f"üßê Cluster de 'Alto Valor/Suspeito' identificado: {high_value_cluster}")

    # 6. Marcar no DataFrame
    df_result = predictions.withColumn("is_anomaly", 
                                       when(col("prediction") == high_value_cluster, lit("SUSPEITA"))
                                       .otherwise(lit("NORMAL"))) \
                           .select("transaction_id", "client_name", "amount", "store_name", "is_anomaly")

    # 7. Salvar tabela de Predi√ß√µes
    print("üíæ Salvando tabela de Intelig√™ncia (Machine Learning)...")
    df_result.write.format("delta").mode("overwrite").save("s3a://gold/ml_fraud_detection")
    
    print("‚úÖ Processo de ML conclu√≠do com sucesso!")
    print("\n--- AMOSTRA DE TRANSA√á√ïES SUSPEITAS ---")
    df_result.filter(col("is_anomaly") == "SUSPEITA").show(5)

except Exception as e:
    print("‚ùå Erro no treinamento:", str(e))

spark.stop()