# ğŸ›¡ï¸ Data Shield Enterprise | End-to-End Data Lakehouse

## ğŸš€ Projeto de Engenharia de Dados com foco em DetecÃ§Ã£o de Fraudes em Tempo Real

O **Data Shield Enterprise** simula uma arquitetura **realista de Big Data usada em fintechs, bancos e empresas de meios de pagamento** para **detecÃ§Ã£o de fraudes financeiras em tempo quase real**.

O objetivo nÃ£o Ã© sÃ³ mover dados, mas **transformar eventos brutos em inteligÃªncia acionÃ¡vel**, aplicando boas prÃ¡ticas modernas de **Engenharia de Dados, Lakehouse e Machine Learning**.

> ğŸ’¡ Pense nisso como um *mini ecossistema de dados corporativo*, rodando 100% em Docker, do evento ao dashboard.

---

## ğŸ§  Problema de NegÃ³cio Simulado

Empresas que processam pagamentos precisam:

* Detectar **transaÃ§Ãµes suspeitas rapidamente**
* Escalar ingestÃ£o de eventos (milhares por segundo)
* Garantir **consistÃªncia, rastreabilidade e histÃ³rico** dos dados
* Disponibilizar **KPIs e alertas em tempo real** para Ã¡reas de risco

Este projeto resolve exatamente isso, simulando um fluxo completo de **anti-fraude orientado a dados**.

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

Arquitetura **Lakehouse com Medallion Architecture**, totalmente containerizada:

### ğŸ”¹ IngestÃ£o (Streaming)

* **Producer:** Script Python (`producer.py`) gerando transaÃ§Ãµes financeiras sintÃ©ticas com **Faker**
* **Mensageria:** **Apache Kafka** para ingestÃ£o distribuÃ­da e alta vazÃ£o
* **Consumo:** **Apache Spark Structured Streaming**

### ğŸ”¹ Armazenamento (Data Lake)

* **MinIO** como Object Storage compatÃ­vel com **Amazon S3**
* **Delta Lake** como formato de armazenamento

  * ACID Transactions
  * Time Travel
  * Schema Enforcement

### ğŸ”¹ Processamento de Dados (ETL)

**Medallion Architecture aplicada na prÃ¡tica:**

* ğŸŸ¤ **Bronze Layer**
  Dados brutos consumidos diretamente do Kafka (eventos como chegam)

* âšª **Silver Layer**
  Limpeza, deduplicaÃ§Ã£o, normalizaÃ§Ã£o de schema e enriquecimento

* ğŸŸ¡ **Gold Layer**
  Dados agregados prontos para consumo analÃ­tico e indicadores de negÃ³cio

### ğŸ”¹ Machine Learning (DetecÃ§Ã£o de Fraudes)

* Modelo **K-Means (Spark MLlib)**
* ClusterizaÃ§Ã£o de transaÃ§Ãµes com base em:

  * Valor
  * FrequÃªncia
  * PadrÃµes comportamentais
* **Outliers identificados como potenciais fraudes**

> âš ï¸ O foco nÃ£o Ã© o modelo em si, mas **como ML se integra ao pipeline de dados em produÃ§Ã£o**.

### ğŸ”¹ VisualizaÃ§Ã£o e Monitoramento

* **Streamlit** para dashboard interativo
* Leitura direta dos dados processados no MinIO (S3)
* KPIs em tempo quase real:

  * Volume de transaÃ§Ãµes
  * DistribuiÃ§Ã£o de valores
  * Alertas de possÃ­veis fraudes

---

## ğŸ§© Arquitetura Visual (Mermaid)

```mermaid
graph LR
    %% Estilos compatÃ­veis com GitHub
    classDef source fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef stream fill:#fff3e0,stroke:#e65100,stroke-width:2px;
    classDef spark fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;
    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px,stroke-dasharray: 5 5;
    classDef viz fill:#ffebee,stroke:#b71c1c,stroke-width:2px;

    %% Fontes de dados
    subgraph Sources [Fonte de Dados]
        Producer[Producer.py
Faker Data]:::source
    end

    %% IngestÃ£o
    subgraph Ingestion [Ingestao & Streaming]
        Kafka[Apache Kafka
Topic: transactions]:::stream
    end

    %% Processamento
    subgraph Processing [Spark Engine & ML]
        SparkBronze[Spark Ingest Bronze]:::spark
        SparkSilver[Spark Process Silver]:::spark
        SparkGold[Spark Process Gold]:::spark
        SparkML[Spark ML Training
K-Means]:::spark
    end

    %% Lakehouse
    subgraph Lakehouse [MinIO Data Lake]
        Bronze[(Bronze Raw Data)]:::storage
        Silver[(Silver Refined Data)]:::storage
        Gold[(Gold Aggregations & ML)]:::storage
    end

    %% Consumo
    subgraph Serving [Visualizacao]
        Dashboard[Streamlit Dashboard]:::viz
    end

    %% Fluxo
    Producer --> Kafka
    Kafka --> SparkBronze
    SparkBronze --> Bronze

    Bronze --> SparkSilver
    SparkSilver --> Silver

    Silver --> SparkGold
    Silver --> SparkML

    SparkGold --> Gold
    SparkML --> Gold

    Gold --> Dashboard
```

---

## ğŸ› ï¸ Stack TecnolÃ³gica

* **Linguagem:** Python 3.8+
* **Processamento:** Apache Spark 3.5 (PySpark)
* **Streaming:** Apache Kafka & Zookeeper
* **Storage:** MinIO (S3 Compatible)
* **Formato:** Delta Lake
* **Machine Learning:** Spark MLlib
* **Infraestrutura:** Docker & Docker Compose
* **VisualizaÃ§Ã£o:** Streamlit & Plotly

---

## ğŸš€ Como Executar o Projeto

### PrÃ©-requisitos

* Docker
* Docker Compose

### 1ï¸âƒ£ Clone o repositÃ³rio

```bash
git clone https://github.com/ricardo-ribeiro12/data-shield-lakehouse.git
cd data-shield-lakehouse
```

### 2ï¸âƒ£ Suba toda a infraestrutura

```bash
docker-compose up -d
```

### 3ï¸âƒ£ Inicie o gerador de eventos

```bash
docker exec -it spark-master python3 /app/producer.py
```

### 4ï¸âƒ£ Execute os pipelines de dados

Os scripts devem ser submetidos via `spark-submit`:

* `ingest_bronze.py` â†’ IngestÃ£o Streaming
* `process_silver.py` â†’ Limpeza e padronizaÃ§Ã£o
* `process_gold.py` â†’ AgregaÃ§Ãµes e KPIs
* `train_model.py` â†’ Treinamento do modelo de fraude

### 5ï¸âƒ£ Visualize o Dashboard

```bash
streamlit run dashboard.py
```

---

## ğŸ“Œ Diferenciais do Projeto

âœ… Arquitetura **end-to-end**, nÃ£o sÃ³ scripts isolados
âœ… Uso real de **Streaming + Lakehouse**
âœ… ML integrado ao pipeline (nÃ£o notebook jogado)
âœ… Infraestrutura reproduzÃ­vel via Docker
âœ… PadrÃ£o prÃ³ximo ao encontrado em empresas de grande porte

---

## ğŸ‘¨â€ğŸ’» Sobre o Autor

**Ricardo Ribeiro**
Engenheiro de Dados | Analytics | Big Data

* GitHub: [https://github.com/ricardo-ribeiro12](https://github.com/ricardo-ribeiro12)
* LinkedIn: [https://www.linkedin.com/in/ricardo-ribeiro12](https://www.linkedin.com/in/ricardo-ribeiro12)

---

> ğŸ’¬ *Este projeto foi desenvolvido com foco em demonstrar capacidade tÃ©cnica prÃ¡tica para vagas de Engenharia de Dados, Analytics Engineer e Ã¡reas correlatas.*
