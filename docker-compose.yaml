version: '3.8'

services:
  # --- 1. MENSAGERIA (KAFKA) ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports: ["2181:2181"]

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on: [zookeeper]
    ports: ["9092:9092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # --- 2. DATA LAKE (MINIO) ---
  minio:
    image: minio/minio
    container_name: minio
    ports: ["9000:9000", "9001:9001"]
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    command: server /data --console-address ":9001"

  # --- 3. FEATURE STORE (REDIS) ---
  redis:
    image: redis:alpine
    container_name: redis
    ports: ["6379:6379"]

  # --- 4. PROCESSAMENTO (SPARK - APACHE OFFICIAL) ---
  spark-master:
    build: 
      context: ./infra/docker
      dockerfile: Dockerfile.spark
    container_name: spark-master
    # Comando explícito para iniciar como Master e liberar acesso externo (0.0.0.0)
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host 0.0.0.0 --port 7077 --webui-port 8080
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_DAEMON_MEMORY=1g
    user: root # Necessário para instalar dependências do Hadoop/AWS
    volumes:
       - ./services/streaming-engine:/app  # <--- CORREÇÃO AQUI: Mapeando o código para o Master

  spark-worker:
    build: 
      context: ./infra/docker
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    depends_on: [spark-master]
    # Conecta no master explicitamente
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    user: root # Necessário para permissões
    volumes:
       - ./services/streaming-engine:/app

  # --- 5. OBSERVABILIDADE ---
  control-center:
    image: confluentinc/cp-enterprise-control-center:7.3.0
    container_name: control-center
    depends_on: [kafka]
    ports: ["9021:9021"]
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka:29092'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      PORT: 9021